{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":108149,"databundleVersionId":13104605,"sourceType":"competition"},{"sourceId":251458427,"sourceType":"kernelVersion"},{"sourceId":251458590,"sourceType":"kernelVersion"},{"sourceId":251458845,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jet Tagging with Graph Neural Networks (GNN)\n\nThis notebook shows how to use a GNN for jet classification between QCD and TT jets using particle-level features represented as a graph. Note that you get some GPU time on Kaggle for free (30 hrs/week). This will make training the model here faster, but note that it won't help you with the graph building, which also does take time here. Sadly you can't turn it on just to run the training, the notebook session must be run as either CPU or GPU. If you want to experiment with GNNs and the graph building becomes annoying, just save the graphs, and then load them as you need. \n\n## What is a GNN?\nA Graph Neural Network (GNN) is a type of neural network designed to process graph-structured data. It can learn from both node features and the graph structure. It can be used to predict information about nodes, links between nodes or graphs as a whole. \n\n## Why use GNNs for Jet Tagging?\n- Very flexible, can start from images or dataframes, or use both \n- Has the potential to remove areas where there is no energy deposited\n- Has the potential to include long range interactions in a way a CNN can't \n- Can handle variable number of particles","metadata":{}},{"cell_type":"code","source":"# this might require you to validate your kaggle account \n# if you don't want that, use binderhub \n!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:43:54.367900Z","iopub.execute_input":"2025-07-24T15:43:54.368402Z","iopub.status.idle":"2025-07-24T15:43:57.606361Z","shell.execute_reply.started":"2025-07-24T15:43:54.368375Z","shell.execute_reply":"2025-07-24T15:43:57.605421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install awkward\n!pip install hist\n!pip install mplhep\n!pip install numpy\n!pip install matplotlib   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:43:59.018212Z","iopub.execute_input":"2025-07-24T15:43:59.018536Z","iopub.status.idle":"2025-07-24T15:44:14.742295Z","shell.execute_reply.started":"2025-07-24T15:43:59.018505Z","shell.execute_reply":"2025-07-24T15:44:14.741502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport hist\nimport mplhep as hep\n\nhep.style.use(\"CMS\")\n\n# Load labels and feature data\nlabels = np.load(\"/kaggle/input/qcd-tt-jet-tagging-co-da-s-hep/train/labels/labels.npy\")\nfeatures_df = pd.read_csv(\"/kaggle/input/qcd-tt-jet-tagging-co-da-s-hep/train/features/cluster_features.csv\")  # adjust path\n\n# Add labels to the DataFrame\nfeatures_df[\"label\"] = labels\n\n# Split by label: 0 = QCD, 1 = TTbar\ndata = {\n    \"QCD\": features_df[features_df[\"label\"] == 0],\n    \"TTbar\": features_df[features_df[\"label\"] == 1]\n}\n\ndef plot_variable(varname, xlabel, sigma_range=3):\n    fig, ax = plt.subplots()\n\n    # Combine values from both classes and filter NaNs\n    combined_values = np.concatenate([\n        data[\"QCD\"][varname].dropna().to_numpy(),\n        data[\"TTbar\"][varname].dropna().to_numpy()\n    ])\n\n    if combined_values.size == 0:\n        print(f\"[WARNING] Skipping {varname} due to no valid data.\")\n        return\n\n    # Get bin count from feature map\n    bin_count = features[varname][1]\n\n    # Mean and std from combined data\n    mean = np.mean(combined_values)\n    std = np.std(combined_values)\n\n    # Define x-range based on µ ± N·σ\n    x_min = np.min(combined_values) #max(np.min(combined_values), mean - sigma_range * std)\n    x_max = np.max(combined_values)#min(np.max(combined_values), mean + sigma_range * std)\n\n    # Bin edges using specified bin count\n    filtered_values = combined_values[(combined_values >= x_min) & (combined_values <= x_max)]\n    bin_edges = np.histogram_bin_edges(filtered_values, bins=bin_count, range=(x_min, x_max))\n\n    # Histogram\n    h = hist.Hist(hist.axis.Variable(bin_edges, name=varname, label=xlabel))\n    max_y = 0\n\n    colors = {\"QCD\": \"tab:blue\", \"TTbar\": \"tab:orange\"}\n\n    for label, df in data.items():\n        values = df[varname].dropna().to_numpy()\n        values = values[(values >= x_min) & (values <= x_max)]\n\n        mean_val = np.mean(values)\n        std_val = np.std(values)\n        entries = len(values)\n\n        weights = np.ones_like(values) / entries if entries > 0 else np.ones_like(values)\n        h.fill(**{varname: values}, weight=weights)\n\n        hep.histplot(\n            h,\n            label=f\"{label} (μ={mean_val:.2f}, σ={std_val:.2f}, N={entries})\",\n            ax=ax,\n            histtype=\"fill\",\n            alpha=0.5,\n            color=colors[label]\n        )\n\n        max_y = max(max_y, np.max(h.values()))\n        h.reset()\n\n    # Set xticks at bin edges and only label clean numbers\n    ax.set_xticks(bin_edges)\n    xtick_labels = [f\"{tick:.2f}\" for tick in bin_edges]\n    ax.set_xticklabels(xtick_labels, rotation=45)\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(\"Normalized Events\")\n    ax.set_xlim(bin_edges[0], bin_edges[-1])\n    ax.set_ylim(0, max_y * 1.1)\n    ax.legend()\n    ax.set_title(f\"{xlabel} (QCD vs TTbar)\")\n    plt.tight_layout()\n    plt.show()\n\n\n# Feature label map\nfeatures = {\n    \"n_clusters\": (\"# clusters per jet\", 10),\n    \"total_pt\": (\"Total cluster pT\", 20),\n    \"max_cluster_pt\": (\"Max cluster pT\", 2),\n    \"mean_cluster_pt\": (\"Mean cluster pT\", 10),\n    \"std_cluster_pt\": (\"Std dev of cluster pT\", 10),\n    \"max_cluster_size\": (\"Max cluster size\", 20),\n    \"mean_cluster_size\": (\"Mean cluster size\", 10),\n    \"std_cluster_size\": (\"Std dev of cluster size\", 10),\n    \"max_cluster_eta\": (\"Max eta of clusters\", 10),\n    \"mean_cluster_eta\": (\"Mean eta of clusters\", 4),\n    \"max_cluster_phi\": (\"Max phi of clusters\", 10),\n    \"mean_cluster_phi\": (\"Mean phi of clusters\", 4),\n    \"cluster_pt_ratio\": (\"Leading/subleading cluster pT ratio\", 2),\n    \"cluster_size_ratio\": (\"Leading/subleading cluster size ratio\", 2)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:44:15.004474Z","iopub.execute_input":"2025-07-24T15:44:15.004722Z","iopub.status.idle":"2025-07-24T15:44:15.451444Z","shell.execute_reply.started":"2025-07-24T15:44:15.004698Z","shell.execute_reply":"2025-07-24T15:44:15.450875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:44:18.122205Z","iopub.execute_input":"2025-07-24T15:44:18.122464Z","iopub.status.idle":"2025-07-24T15:44:18.143500Z","shell.execute_reply.started":"2025-07-24T15:44:18.122445Z","shell.execute_reply":"2025-07-24T15:44:18.142782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for var, (label, _) in features.items():  \n    if var in features_df.columns:\n        plot_variable(var, label) \n    else:\n        print(f\"[WARNING] {var} not found in CSV columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:44:21.413523Z","iopub.execute_input":"2025-07-24T15:44:21.413813Z","iopub.status.idle":"2025-07-24T15:44:26.388784Z","shell.execute_reply.started":"2025-07-24T15:44:21.413790Z","shell.execute_reply":"2025-07-24T15:44:26.388156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load and Prepare Data\n\nWe will create the graphs from images, but there are many other ways to do it. \n\nThe energy deposited in a pixel will be the node feature, and we will connect each pixel to its eight closest pixels (up, down, sides, diagonals). \n\nThe graph building function written here has an option to set max_nodes, which will only choose the top n most energetic nodes. There's also an option whether to consider all nodes or not, if you set this to false, it will throw away all the pixels with 0 energy deposited. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.data import Data, DataLoader\nfrom jet_utils import load_images\nfrom jet_gnn_utils import create_graph_data\nfrom jet_plotting_utils import plot_confusion_matrix, plot_training_history, plot_roc_curve","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:46:15.600730Z","iopub.execute_input":"2025-07-24T15:46:15.601031Z","iopub.status.idle":"2025-07-24T15:46:15.605498Z","shell.execute_reply.started":"2025-07-24T15:46:15.600982Z","shell.execute_reply":"2025-07-24T15:46:15.604709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\nX_train, y_train, train_ids, X_val, y_val, val_ids, X_test, test_ids = load_images()\n# Convert to graph format - this might take a bit of time to run\nX_train_graphs = create_graph_data(X_train, y_train, max_nodes=900, consider_all_nodes=True)\nX_val_graphs = create_graph_data(X_val, y_val, max_nodes=900)\nX_test_graphs = create_graph_data(X_test, max_nodes=900)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:46:16.426453Z","iopub.execute_input":"2025-07-24T15:46:16.426714Z","iopub.status.idle":"2025-07-24T15:50:52.775017Z","shell.execute_reply.started":"2025-07-24T15:46:16.426694Z","shell.execute_reply":"2025-07-24T15:50:52.774246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Build GNN Model\n\nWe'll create a GNN with:\n- Graph Convolutional layers to learn from node features and graph structure\n- Global pooling to get graph-level representations\n- Dense layers for classification","metadata":{}},{"cell_type":"code","source":"class GNN(nn.Module):\n    def __init__(self, num_features):\n        super(GNN, self).__init__()\n        # Graph convolution layers\n        self.conv1 = GCNConv(num_features, 64)\n        self.conv2 = GCNConv(64, 64)\n        self.conv3 = GCNConv(64, 64)\n\n        \n        # Dense layers\n        self.fc1 = nn.Linear(64, 32)\n        self.fc2 = nn.Linear(32, 16)\n        self.fc3 = nn.Linear(16, 1)\n        \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        # Graph convolution layers\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.relu(self.conv2(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.relu(self.conv3(x, edge_index))\n        \n        # Global pooling\n        x = global_mean_pool(x, batch)\n        \n        # Dense layers\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=0.5, training=self.training)\n        #x = self.fc2(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x #torch.sigmoid(x)\n\n# Create model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = GNN(num_features=1).to(device)  # 4 features: pt, eta, phi, charge\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.BCEWithLogitsLoss() #nn.BCELoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:50:52.776437Z","iopub.execute_input":"2025-07-24T15:50:52.776715Z","iopub.status.idle":"2025-07-24T15:50:52.788805Z","shell.execute_reply.started":"2025-07-24T15:50:52.776691Z","shell.execute_reply":"2025-07-24T15:50:52.788032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(X_train_graphs, batch_size=32, shuffle=True)\nval_loader = DataLoader(X_val_graphs, batch_size=32, shuffle=True)\ntest_loader = DataLoader(X_test_graphs, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:51:23.212908Z","iopub.execute_input":"2025-07-24T15:51:23.213516Z","iopub.status.idle":"2025-07-24T15:51:23.219797Z","shell.execute_reply.started":"2025-07-24T15:51:23.213490Z","shell.execute_reply":"2025-07-24T15:51:23.218996Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Train Model\n\nWe'll train the GNN with:\n- Adam optimizer\n- Binary Cross Entropy loss\n- Early stopping based on validation accuracy\n\n\nYou'll see this code differs a fair bit from the previous DNN and CNN. This is because we're using pytorch geometric -  the most common GNN package. It integrates with Pytorch, so we are using Pytorch instead of keras. Torch is growing to be the most popular ML libary, but keras is easier to learn. \n\nYou might also see that training on CPU this is a lot slower than the other ML methods. Can you explain why? ","metadata":{}},{"cell_type":"code","source":"def train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        optimizer.zero_grad()\n        out = model(data.to(device))\n        loss = criterion(out, data.y.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\n\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        out = model(data.to(device))\n        loss = criterion(out, data.y.view(-1, 1))\n        pred = (out > 0.5).float()\n        correct += int((pred == data.y.view(-1, 1)).sum())\n    return correct / len(loader.dataset), loss\n\n\nhistory = {\n    'loss': [],\n    'val_loss': [],\n    'accuracy': [],\n    'val_accuracy': []\n}\n\n\n# Training loop\nbest_acc = 0\nfor epoch in range(70):\n    loss = train()\n    train_acc = test(train_loader)[0]\n    val_acc, val_loss = test(val_loader)\n    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pt')\n\n\n    # Append to history\n    history['loss'].append(loss)\n    history['val_loss'].append(val_loss)\n    history['accuracy'].append(train_acc)\n    history['val_accuracy'].append(val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:51:25.119995Z","iopub.execute_input":"2025-07-24T15:51:25.120699Z","iopub.status.idle":"2025-07-24T15:55:18.480986Z","shell.execute_reply.started":"2025-07-24T15:51:25.120676Z","shell.execute_reply":"2025-07-24T15:55:18.480136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Evaluate Model\n\nLet's evaluate our model's performance on the test set.","metadata":{}},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('best_model.pt'))\n\n# Evaluate on test set\nmodel.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad(): \n    for data in val_loader:\n        out = model(data.to(device))\n        y_true.extend(data.to('cpu').y.numpy())\n        y_pred.extend(out.to('cpu').numpy())\n\ny_pred = np.array([x[0] for x in y_pred])\npred_discrete = np.where(y_pred > 0.5, 1, 0)\n# Plot confusion matrix\nplot_confusion_matrix(y_true, pred_discrete)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:55:20.243848Z","iopub.execute_input":"2025-07-24T15:55:20.244653Z","iopub.status.idle":"2025-07-24T15:55:20.710870Z","shell.execute_reply.started":"2025-07-24T15:55:20.244628Z","shell.execute_reply":"2025-07-24T15:55:20.710056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_roc_curve(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:55:22.742170Z","iopub.execute_input":"2025-07-24T15:55:22.742679Z","iopub.status.idle":"2025-07-24T15:55:22.990767Z","shell.execute_reply.started":"2025-07-24T15:55:22.742657Z","shell.execute_reply":"2025-07-24T15:55:22.990078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 - Making predictions","metadata":{}},{"cell_type":"code","source":"y_pred_test = []\nfor data in test_loader:\n    with torch.no_grad():\n        output = model(data.to(device))\n        # could you change the prediction threshold? Would that make it better?\n        y_pred_test.extend(output.to('cpu').numpy())\n\ny_pred_test = np.array([x[0] for x in y_pred_test])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:55:28.160175Z","iopub.execute_input":"2025-07-24T15:55:28.160852Z","iopub.status.idle":"2025-07-24T15:55:28.555820Z","shell.execute_reply.started":"2025-07-24T15:55:28.160828Z","shell.execute_reply":"2025-07-24T15:55:28.554895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nsolution = pd.DataFrame({'id':test_ids, 'label':y_pred_test})\nsolution.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T15:55:30.689594Z","iopub.execute_input":"2025-07-24T15:55:30.690141Z","iopub.status.idle":"2025-07-24T15:55:30.698418Z","shell.execute_reply.started":"2025-07-24T15:55:30.690117Z","shell.execute_reply":"2025-07-24T15:55:30.697781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}